{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../utils\")\n",
    "from constants import *\n",
    "from prompts_science import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2int = {\n",
    "    \"Incorrect\": -1,\n",
    "    \"Misleading\": 0,\n",
    "    \"Correct\": 1\n",
    "}\n",
    "\n",
    "path = f\"{gdrive_path}/data/sciencefeedback.jsonl\"\n",
    "df = pd.read_json(path, orient=\"records\", lines=True)\n",
    "cv = df.loc[df[\"topic\"] == \"Health\", [\"topic\", \"claim\", \"verdict\"]].values[:150]\n",
    "topics, claims, verdicts = cv[:, 0], cv[:, 1], cv[:, 2]\n",
    "ids = np.arange(len(claims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "llama3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 100%|██████████| 150/150 [00:00<00:00, 157.09it/s]\n",
      "compare: 100%|██████████| 22500/22500 [03:08<00:00, 119.62it/s]\n",
      "contrast:1: 100%|██████████| 22500/22500 [04:03<00:00, 92.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mistral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "contrast:2: 100%|██████████| 22500/22500 [05:09<00:00, 72.70it/s] \n",
      "score: 100%|██████████| 150/150 [00:00<00:00, 680.34it/s]\n",
      "compare: 100%|██████████| 22500/22500 [04:05<00:00, 91.55it/s]\n",
      "contrast:1: 100%|██████████| 22500/22500 [03:38<00:00, 103.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "contrast:2: 100%|██████████| 22500/22500 [03:36<00:00, 103.85it/s]\n",
      "score: 100%|██████████| 150/150 [00:00<00:00, 636.37it/s]\n",
      "compare: 100%|██████████| 22500/22500 [03:48<00:00, 98.39it/s]\n",
      "contrast:1: 100%|██████████| 22500/22500 [05:15<00:00, 71.24it/s] \n"
     ]
    }
   ],
   "source": [
    "for model in [\"llama3\", \"mistral\", \"llama2\"]:\n",
    "    print(f\"\\n{model}\")\n",
    "\n",
    "    # scoring\n",
    "    prompts = pd.DataFrame(columns=[\"claimID\", \"topic\", \"claim\", \"verdict\", \"prompt\"])\n",
    "    bar = tqdm(total=len(ids), desc=\"score\")\n",
    "    for id, topic, claim, verdict in zip(ids, topics, claims, verdicts):\n",
    "        bar.update()\n",
    "        prompt = chat_templates[model].format(\n",
    "            INSTRUCTION=template_score.format(CLAIM=claim),\n",
    "            ANSWER=\"The above claim is \"\n",
    "        )\n",
    "        prompts.loc[len(prompts)] = [id, topic, claim, verdict, prompt]\n",
    "    prompts.to_json(f\"{gdrive_path}/prompts_short/{model}/healthfeedback_score.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "    # comparison\n",
    "    prompts = pd.DataFrame(columns=[\"claimID\", \"topic\", \"prompt\", \"P(Claim 1)\"])\n",
    "    pairs = []\n",
    "    bar = tqdm(total=len(ids)**2, desc=\"compare\")\n",
    "    for topic, id1, claim1, v1 in zip(topics, ids, claims, verdicts):\n",
    "        if topic != \"Health\": continue\n",
    "        for topic2, id2, claim2, v2 in zip(topics, ids, claims, verdicts):\n",
    "            bar.update()\n",
    "            if topic != topic2: continue\n",
    "            if id1 == id2: continue\n",
    "            if (id1, id2) not in pairs:\n",
    "                pairs.append((id1, id2))\n",
    "                pairs.append((id1, id2))\n",
    "                row = [(id1, id2), topic]\n",
    "                prompt = chat_templates[model].format(\n",
    "                    INSTRUCTION=template_compare.format(\n",
    "                        CLAIM1=claim1,\n",
    "                        CLAIM2=claim2\n",
    "                    ),\n",
    "                    ANSWER=\"Between Claim 1 and Claim 2, the more factually accurate / less ambiguous choice is Claim \"\n",
    "                )\n",
    "                row.append(prompt)\n",
    "                if v2int[v1] > v2int[v2]:\n",
    "                    row.append(1)\n",
    "                elif v2int[v1] < v2int[v2]:\n",
    "                    row.append(0)\n",
    "                else:\n",
    "                    row.append(0.5)\n",
    "                prompts.loc[len(prompts)] = row\n",
    "    prompts.to_json(f\"{gdrive_path}/prompts_short/{model}/healthfeedback_compare.jsonl\", orient=\"records\", lines=True) \n",
    "\n",
    "    # contrast pairs\n",
    "    for choice in [1, 2]:\n",
    "        prompts = pd.DataFrame(columns=[\"claimID\", \"topic\", \"prompt\", \"P(Claim 1)\"])\n",
    "        pairs = []\n",
    "        bar = tqdm(total=len(ids)**2, desc=f\"contrast:{choice}\")\n",
    "        for topic, id1, claim1, v1 in zip(topics, ids, claims, verdicts):\n",
    "            if topic != \"Health\": continue\n",
    "            for topic2, id2, claim2, v2 in zip(topics, ids, claims, verdicts):\n",
    "                bar.update()\n",
    "                if topic != topic2: continue\n",
    "                if id1 == id2: continue\n",
    "                if (id1, id2) not in pairs:\n",
    "                    pairs.append((id1, id2))\n",
    "                    pairs.append((id1, id2))\n",
    "                    row = [(id1, id2), topic]\n",
    "                    prompt = chat_templates[model].format(\n",
    "                        INSTRUCTION=template_compare.format(\n",
    "                            CLAIM1=claim1,\n",
    "                            CLAIM2=claim2\n",
    "                        ),\n",
    "                        ANSWER=f\"Between Claim 1 and Claim 2, the more factually accurate / less ambiguous choice is Claim {choice}\"\n",
    "                    )\n",
    "                    row.append(prompt)\n",
    "                    if v2int[v1] > v2int[v2]:\n",
    "                        row.append(1)\n",
    "                    elif v2int[v1] < v2int[v2]:\n",
    "                        row.append(0)\n",
    "                    else:\n",
    "                        row.append(0.5)\n",
    "                    prompts.loc[len(prompts)] = row\n",
    "        prompts.to_json(f\"{gdrive_path}/prompts_short/{model}/healthfeedback_contrast_{choice}.jsonl\", orient=\"records\", lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pairs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
