{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PPairS.prompts import chat_templates, aspect_noun2adj, bm_instructions, bm_theirs_compare, bm_theirs_score, bm_mine_compare\n",
    "import pandas as pd\n",
    "from typing import Callable, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comparisons(\n",
    "        summaries: pd.DataFrame,\n",
    "        chat_template: str,\n",
    "        instruction: Callable[[str, str, str, str, str], str],\n",
    "        answer: Callable[[str, str], str],\n",
    "        choice: Optional[int]=None\n",
    ") -> pd.DataFrame:\n",
    "    '''\n",
    "    summaries: dataframe generated in process_benchmark_datasets.py\n",
    "    chat_template: see `chat_templates` in prompts.py\n",
    "    instruction: function which takes a general instruction, article, two summaries and an aspect\n",
    "                 and returns the instruction prompt.\n",
    "    answer: function which takes an aspect and a choice (for contrast-pairs) \n",
    "            and returns the initial answer prompt for the assistant.\n",
    "    choice: for contrast-pairs\n",
    "    '''\n",
    "    \n",
    "    aspects = [\"coherence\", \"consistency\", \"fluency\", \"relevance\"]\n",
    "    columns = [\"article_id\", \"summary_id\", \"article\", \"summary1\", \"summary2\"]\n",
    "    columns += aspects + [f\"prompt_{aspect}\" for aspect in aspects]\n",
    "\n",
    "    comparisons = pd.DataFrame(columns=columns)\n",
    "    # for each article...\n",
    "    for article in tqdm(summaries.article_id.unique()):\n",
    "        subset = summaries.loc[summaries.article_id == article, :]\n",
    "        # for every pair of summaries...\n",
    "        for summary1_id in subset[\"summary_id\"].unique():\n",
    "            for summary2_id in subset[\"summary_id\"].unique():\n",
    "                row = [article, (summary1_id, summary2_id), subset[\"article\"].iloc[0]]\n",
    "                summary1 = subset.loc[subset[\"summary_id\"] == summary1_id, \"summary\"].item()\n",
    "                summary2 = subset.loc[subset[\"summary_id\"] == summary2_id, \"summary\"].item()\n",
    "                row.append(summary1)\n",
    "                row.append(summary2)\n",
    "                prompts = []\n",
    "                # grab the pairs of scores corresponding to each summary in the pair\n",
    "                for aspect in aspects:\n",
    "                    s1 = subset.loc[subset[\"summary_id\"] == summary1_id, aspect].item()\n",
    "                    s2 = subset.loc[subset[\"summary_id\"] == summary2_id, aspect].item()\n",
    "                    row.append((round(s1, 2), round(s2, 2)))\n",
    "                    # construct the test prompt\n",
    "                    inst = instruction(\n",
    "                        bm_instructions[aspect],\n",
    "                        subset[\"article\"].iloc[0],\n",
    "                        summary1,\n",
    "                        summary2,\n",
    "                        aspect_noun2adj[aspect]\n",
    "                    )\n",
    "                    ans = answer(aspect_noun2adj[aspect], choice)\n",
    "                    prompt = chat_template.format(\n",
    "                        INSTRUCTION=inst,\n",
    "                        ANSWER=ans\n",
    "                    )\n",
    "                    prompts.append(prompt)\n",
    "                row.extend(prompts)\n",
    "                comparisons.loc[len(comparisons)] = row\n",
    "    return comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_theirs = lambda inst, article, s1, s2, aspect: bm_theirs_compare.format(\n",
    "    INSTRUCTION=inst,\n",
    "    ARTICLE=article,\n",
    "    SUMMARY1=s1,\n",
    "    SUMMARY2=s2,\n",
    "    ASPECT=aspect\n",
    ")\n",
    "answer_theirs = lambda _1, _2: \"Answer: \"\n",
    "\n",
    "instruction_mine = lambda _, article, s1, s2, aspect: bm_mine_compare.format(\n",
    "    ARTICLE=article,\n",
    "    SUMMARY1=s1,\n",
    "    SUMMARY2=s2,\n",
    "    ASPECT=aspect\n",
    ")\n",
    "answer_mine = lambda aspect, choice: f\"Between Choice 1 and Choice 2, the more {aspect} summary is Choice {choice}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [\"summeval\", \"newsroom\"]    \n",
    "    for model in [\"mistral\", \"llama2\", \"llama3\"]:\n",
    "\n",
    "        path = f\"{gdrive_path}/data/{dataset}-processed.jsonl\"\n",
    "        summaries = pd.read_json(path, orient=\"records\", lines=True)\n",
    "\n",
    "        # pairwise comparisons\n",
    "        path = f\"{gdrive_path}/prompts/{model}/{dataset}\"\n",
    "        chat_template = chat_templates[model]\n",
    "        for i in range(2):\n",
    "            instruction = [instruction_theirs, instruction_mine][i]\n",
    "            answer = [answer_theirs, answer_mine][i]\n",
    "            name = [\"theirs\", \"mine\"][i]\n",
    "            filename = f\"{path}_{name}\"\n",
    "            args = [summaries, chat_template, instruction, answer]\n",
    "            if name == \"mine\":\n",
    "                for choice in [1, 2]:\n",
    "                    comparisons = get_comparisons(*args, choice)\n",
    "                    comparisons.to_json(f\"{filename}_{choice}.jsonl\", orient=\"records\", lines=True)\n",
    "            else:\n",
    "                comparisons = get_comparisons(*args)\n",
    "                comparisons.to_json(f\"{filename}.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "        # direct-scoring\n",
    "        for aspect in [\"coherence\", \"consistency\", \"fluency\", \"relevance\"]:\n",
    "            prompts = []\n",
    "            for _, row in summaries.iterrows():\n",
    "                prompt = theirs_score.format(\n",
    "                    INSTRUCTION=theirs_instructions[aspect],\n",
    "                    ARTICLE=row[\"article\"],\n",
    "                    SUMMARY=row[\"summary\"],\n",
    "                    ASPECT=aspect_noun2adj[aspect]\n",
    "                )\n",
    "                prompt = chat_templates[model].format(\n",
    "                    INSTRUCTION=prompt,\n",
    "                    ANSWER=\"Score: \"\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            summaries[f\"prompt_{aspect}\"] = prompts\n",
    "        summaries.to_json(f\"{gdrive_path}/prompts/{model}/{dataset}_score.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "        clear_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pairs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
