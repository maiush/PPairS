{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, dill\n",
    "from dev.constants import gdrive_path\n",
    "\n",
    "import torch as t\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "from functools import reduce\n",
    "from random import shuffle, choices\n",
    "from jaxtyping import Float\n",
    "from typing import Any, Optional, List, Tuple\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPCCDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            subset_reports: Optional[List[str]]=[\"AR3\", \"AR4\", \"AR5\", \"AR6\"],\n",
    "            subset_parts: Optional[List[int]]=None,\n",
    "            device: str=\"cuda\"\n",
    "    ):\n",
    "        self.device = t.device(device)\n",
    "\n",
    "        # all prompts used: 1-1 correspondence with activations\n",
    "        self.files = os.listdir(f\"{gdrive_path}/ipcc_tagging/prompts\")\n",
    "        if subset_reports is not None:\n",
    "            self.files = [f for report in subset_reports for f in self.files if report in f]\n",
    "        if subset_parts is not None:\n",
    "            self.files = [f for part in subset_parts for f in self.files if f\"_PART{part}.jsonl\" in f]\n",
    "\n",
    "        # we index over all harvested activations as one dataset\n",
    "        self.file_ranges = []\n",
    "        # we will cache all activations and labels for faster data loading\n",
    "        self.act_cache, self.labels_cache, self.id_cache, ix = {}, {}, {}, 0\n",
    "        for file in tqdm(self.files, desc=\"caching data\"):\n",
    "            data = t.load(f\"{gdrive_path}/ipcc_tagging/activations/{file.replace('jsonl', 'pt')}\", pickle_module=dill)\n",
    "            prompts = pd.read_json(f\"{gdrive_path}/ipcc_tagging/prompts/{file}\", orient=\"records\", lines=True)\n",
    "            ids = prompts.apply(lambda row: (row[\"S1\"], row[\"S2\"]), axis=1).to_numpy()\n",
    "            labels = t.from_numpy(prompts[\"P(S1)\"].to_numpy())\n",
    "\n",
    "            self.act_cache[file] = data.to(self.device)\n",
    "            self.labels_cache[file] = labels.to(data.dtype).to(self.device)\n",
    "            self.id_cache[file] = ids\n",
    "\n",
    "            N = data.size(0)\n",
    "            self.file_ranges.append((ix, ix+N))\n",
    "            ix += N\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.file_ranges[-1][1]\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        file_ix = next(i for i, (start, end) in enumerate(self.file_ranges) if start <= ix < end)\n",
    "        file = self.files[file_ix]\n",
    "        data_ix = ix - self.file_ranges[file_ix][0]\n",
    "        \n",
    "        data = self.act_cache[file][data_ix]\n",
    "        label = self.labels_cache[file][data_ix]\n",
    "        return data, label\n",
    "\n",
    "    def get_splits(self, splits: Tuple[float]=(0.8, 0.9)) -> Tuple[Subset]:\n",
    "        all_train_ixs, all_val_ixs, all_test_ixs = [], [], []\n",
    "        # for each report\n",
    "        for i in range(1, 7):\n",
    "            report = f\"AR{i}\"\n",
    "\n",
    "            # split the IDs into train/val/test sets\n",
    "            files = [filename for filename in self.files if report in filename]\n",
    "            if len(files) == 0: continue\n",
    "            ids = [self.id_cache[filename] for filename in files]\n",
    "            ids = map(lambda ids: set([pair[0] for pair in ids]), ids)\n",
    "            ids = list(reduce(lambda x, y: x.union(y), ids))\n",
    "            shuffle(ids)\n",
    "            trainval_split, valtest_split = int(splits[0]*len(ids)), int(splits[1]*len(ids))\n",
    "            train_ids = ids[:trainval_split]\n",
    "            val_ids = ids[trainval_split:valtest_split]\n",
    "            test_ids = ids[valtest_split:]\n",
    "\n",
    "            # find the corresponding data indices for the above IDs\n",
    "            train_ixs, val_ixs, test_ixs = [], [], []\n",
    "            for filename in files:\n",
    "                file_ix = self.files.index(filename)\n",
    "                start, _ = self.file_ranges[file_ix]\n",
    "                train_ixs.extend([start+i for i, x in enumerate(self.id_cache[filename]) if x[0] in train_ids])\n",
    "                val_ixs.extend([start+i for i, x in enumerate(self.id_cache[filename]) if x[0] in val_ids])\n",
    "                test_ixs.extend([start+i for i, x in enumerate(self.id_cache[filename]) if x[0] in test_ids])\n",
    "\n",
    "            all_train_ixs.append(train_ixs)\n",
    "            all_val_ixs.append(val_ixs)\n",
    "            all_test_ixs.append(test_ixs)\n",
    "\n",
    "        # resample from under-sampled reports\n",
    "        for group in [all_train_ixs, all_val_ixs, all_test_ixs]:\n",
    "            N = max([len(x) for x in group])\n",
    "            for i in range(len(group)):\n",
    "                ixs = group[i]\n",
    "                delta = N - len(ixs)\n",
    "                if delta > 0: \n",
    "                    group[i] = ixs + choices(ixs, k=delta)\n",
    "\n",
    "        # create subsets for each dataset\n",
    "        all_train_ixs = [ix for ixs in all_train_ixs for ix in ixs]\n",
    "        all_val_ixs = [ix for ixs in all_val_ixs for ix in ixs]\n",
    "        all_test_ixs = [ix for ixs in all_test_ixs for ix in ixs]\n",
    "        train_dataset = Subset(self, all_train_ixs)\n",
    "        val_dataset = Subset(self, all_val_ixs)\n",
    "        test_dataset = Subset(self, all_test_ixs)\n",
    "\n",
    "        return (train_dataset, val_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_class: Any,\n",
    "        dataset_kwargs: dict={},\n",
    "        splits: Tuple[float]=(0.8, 0.9),\n",
    "        batch_size: int=64\n",
    "    ):\n",
    "        # dataset and subsets\n",
    "        self.dataset = dataset_class(**dataset_kwargs)\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = self.dataset.get_splits(splits)\n",
    "        # dataloaders\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.val_loader = DataLoader(self.val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPairSClassifier(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int=4096,\n",
    "        d_out: int=1,\n",
    "        dtype: t.dtype=t.float32,\n",
    "        device: str=\"cuda\"\n",
    "    ):\n",
    "        super(PPairSClassifier, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_out = d_out\n",
    "        self.dtype = dtype\n",
    "        self.device = t.device(device)\n",
    "\n",
    "        self.probe = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_out,\n",
    "            bias=True,\n",
    "            dtype=self.dtype,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Float[Tensor, \"batch d_model\"]\n",
    "    ) -> Float[Tensor, \"d_out\"]:\n",
    "        return self.probe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    dataset_class=IPCCDataset,\n",
    "    dataset_kwargs={\n",
    "        \"subset_reports\": [\"AR3\", \"AR4\", \"AR5\", \"AR6\"],\n",
    "        \"subset_parts\": [1, 2, 3],\n",
    "        \"device\": \"cpu\"\n",
    "    },\n",
    "    splits=(0.8, 0.9),\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate weights for classification\n",
    "totalZ, totalH, totalO = 0, 0, 0\n",
    "for batch, labels in trainer.train_loader:\n",
    "    totalZ += (labels == 0).sum()\n",
    "    totalH += (labels == 0.5).sum()\n",
    "    totalO += (labels == 1).sum()\n",
    "total = totalZ + totalH + totalO\n",
    "w = t.stack([(totalH + totalO) / totalZ, (totalZ + totalO) / totalH, (totalZ + totalH) / totalO])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epoch, d_out = 100, 1\n",
    "\n",
    "# clfs = [PPairSClassifier() for _ in range(3)]\n",
    "# opts = [t.optim.Adam(clf.parameters(), lr=0.01, weight_decay=0.) for clf in clfs]\n",
    "# for epoch in range(n_epoch):\n",
    "#     for batch, labels in trainer.train_loader:\n",
    "#         y = [\n",
    "#             (labels == 0.),\n",
    "#             (labels == 0.5),\n",
    "#             (labels == 1.)\n",
    "#         ]\n",
    "#         for i in range(3):\n",
    "#             logits = clfs[i](batch)\n",
    "#             loss = F.binary_cross_entropy_with_logits(logits.squeeze(-1), y[i], pos_weight=w[i])\n",
    "#             opts[i].zero_grad()\n",
    "#             loss.backward()\n",
    "#             opts[i].step()\n",
    "\n",
    "#     accuracy = []\n",
    "#     for batch, labels in trainer.val_loader:\n",
    "#         class_ixs = (labels * 2.)\n",
    "#         with t.inference_mode(): logits = t.stack([clf(batch) for clf in clfs]).squeeze(-1)\n",
    "#         predictions = logits.argmax(dim=0)\n",
    "#         accuracy.append((predictions == class_ixs).float().mean())\n",
    "#     accuracy = sum(accuracy) / len(accuracy)\n",
    "#     print(f\"epoch {epoch+1}: {round(accuracy.item(), 3)}\")\n",
    "    \n",
    "#     if accuracy > 0.8: break\n",
    "\n",
    "# accuracy = []\n",
    "# for batch, labels in trainer.test_loader:\n",
    "#     class_ixs = (labels * 2.)\n",
    "#     with t.inference_mode(): logits = t.stack([clf(batch) for clf in clfs]).squeeze(-1)\n",
    "#     predictions = logits.argmax(dim=0)\n",
    "#     accuracy.append((predictions == class_ixs).float().mean())\n",
    "# accuracy = sum(accuracy) / len(accuracy)\n",
    "# print(\"-\"*50)\n",
    "# print(f\"test accuracy: {round(accuracy.item(), 3)}\")\n",
    "\n",
    "# path = \"/gws/nopw/j04/ai4er/users/maiush/PPairS/src/dev/_ipcc\"\n",
    "# t.save(clfs[0].state_dict(), f\"{path}/clf_zero.pt\", pickle_module=dill)\n",
    "# t.save(clfs[1].state_dict(), f\"{path}/clf_half.pt\", pickle_module=dill)\n",
    "# t.save(clfs[2].state_dict(), f\"{path}/clf_one.pt\", pickle_module=dill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4096])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for batch, labels in trainer.train_loader:\n",
    "    break\n",
    "\n",
    "print(batch.shape)\n",
    "print(labels.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pairs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
